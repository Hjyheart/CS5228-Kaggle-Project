{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5228-2410 Final Project EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "*(from [Kaggle Project](https://www.kaggle.com/competitions/cs-5228-2410-final-project))*\n",
    "\n",
    "In this project, we look into the market for used cars in Singapore. Car ownership in Singapore is rather expensive which includes the very high prices for new and used cars (compared to many other countries). There are many stakeholders in this market. Buyers and sellers want to find good prices, so they need to understand what affects the value of a car. Online platforms facilitating the sale of used cars, on the other hand, want to maximize the number of sales/transactions.\n",
    "\n",
    "The goal of this task is to predict the resale price of a car based on its properties (e.g., make, model, mileage, age, power, etc). It is therefore first and foremost a regression task. These different types of information allow you to come up with features for training a regressor. It is part of the project for you to justify, derive and evaluate different features. Besides predicting the outcome in terms of a dollar value, other useful results include the importance of different attributes, the evaluation and comparison of different regression techniques, an error analysis and discussion about limitations and potential extensions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Steps\n",
    "1. Load dataset\n",
    "2. Initial inspection\n",
    "    - Basic information\n",
    "    - Summary stats\n",
    "3. Data quality check\n",
    "    - Missing values\n",
    "    - Identify duplicates\n",
    "    - Examine data types\n",
    "4. EDA\n",
    "    - Univariate analysis\n",
    "        - Numerical: histograms, boxplots\n",
    "        - Categorical: barplots, pie charts\n",
    "    - Bivariate analysis\n",
    "        - Scatterplots: price v numerical\n",
    "        - Boxplots: price v categorical\n",
    "    - Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n",
    "import difflib\n",
    "import dateutil\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('visualisations/'):\n",
    "    os.mkdir('visualisations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1    Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2    Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of records: \",len(df_train))\n",
    "print(\"# of columns: \",len(df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of records: \",len(df_test))\n",
    "print(\"# of columns: \",len(df_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3    Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "def get_missing_info(df_train):\n",
    "    missing_values = df_train.isnull().sum()\n",
    "    missing_percentages = (missing_values / len(df_train) * 100).round(2)\n",
    "\n",
    "    missing_info = pd.DataFrame({\n",
    "        'missing_count': missing_values,\n",
    "        'missing_perc': missing_percentages\n",
    "    }).sort_values(by=\"missing_count\", ascending=False)\n",
    "\n",
    "    return missing_info\n",
    "\n",
    "missing_info = get_missing_info(df_train=df_train)\n",
    "print(\"\\n=== missing_count Analysis ===\")\n",
    "print(missing_info[missing_info['missing_count'] > 0].sort_values('missing_perc', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has significant missing data across several features:\n",
    "\n",
    "- 'indicative_price' is completely missing (100%)\n",
    "- 'opc_scheme' and 'original_reg_date' are missing for nearly all records (>98%)\n",
    "- 'lifespan' is missing for about 90% of the records\n",
    "- 'fuel_type' is missing for about 76% of the records\n",
    "- 'mileage' is missing for about 21% of the records\n",
    "- Several other fields have missing data ranging from 0.03% to 15.25%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Duplicate Records Analysis ===\")\n",
    "print(f\"Number of duplicate rows: {df_train.duplicated().sum()}\")\n",
    "print(f\"Number of duplicate listing_ids: {df_train['listing_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no duplicate rows or listing_ids in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Since `indicative_price` is completely missing, drop column. \n",
    "- `listing_id` is not meaningful to the analysis as well. \n",
    "- Since `original_reg_date` is almost entirely missing, feature is not meaningful. Based on context, last `reg_date` may be more useful as it may be closely related to the COE price upon time of registration. COE has a heavy influence on car resale price.\n",
    "- `fuel_type` has many missing values but can potentially be obtained from `category`.\n",
    "- `lifespan` has many missing values but can potentially be inferred from the `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_DROP = ['indicative_price']\n",
    "\n",
    "df_train = df_train.drop(columns=COLS_TO_DROP)\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model make quality\n",
    "unique_makes = df_train['make'].unique()\n",
    "unique_models = df_train['model'].unique()\n",
    "\n",
    "make_model_pairs = df_train.groupby('make')['model'].apply(set)\n",
    "model_count = df_train['model'].value_counts()\n",
    "make_count = df_train['make'].value_counts()\n",
    "\n",
    "# 4. Check for missing values\n",
    "missing_makes = df_train['make'].isnull().sum()\n",
    "missing_models = df_train['model'].isnull().sum()\n",
    "\n",
    "# 5. Text normalization\n",
    "df_train['make'] = df_train['make'].str.strip().str.title()  # Example normalization\n",
    "df_train['model'] = df_train['model'].str.strip().str.title()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Unique Makes: {unique_makes}\")\n",
    "print(f\"Unique Models: {unique_models}\")\n",
    "print(\"Make-Model Pairs:\")\n",
    "print(make_model_pairs)\n",
    "print(f\"Missing Makes: {missing_makes}, Missing Models: {missing_models}\")\n",
    "print(model_count[:10])\n",
    "print(make_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check manufactured year\n",
    "# convert year to type int\n",
    "df_train['manufactured'] = pd.to_numeric(df_train['manufactured'], errors='coerce').astype('Int64')\n",
    "print(df_train['manufactured'].describe())\n",
    "\n",
    "# Check for future years\n",
    "current_year = pd.Timestamp.now().year\n",
    "if (df_train['manufactured'] > current_year).any():\n",
    "    print(\"There are future years of manufacture\")\n",
    "else:\n",
    "    print(\"There are no future years of manufacture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format (assuming they are in string format)\n",
    "df_train['original_reg_date'] = pd.to_datetime(df_train['original_reg_date'], errors='coerce')\n",
    "df_train['reg_date'] = pd.to_datetime(df_train['reg_date'], errors='coerce')\n",
    "\n",
    "# Check for Missing Values\n",
    "missing_original_reg_date = df_train['original_reg_date'].isnull().sum()\n",
    "missing_reg_date = df_train['reg_date'].isnull().sum()\n",
    "\n",
    "# Check Data Types\n",
    "original_reg_date_type = df_train['original_reg_date'].dtype\n",
    "reg_date_type = df_train['reg_date'].dtype\n",
    "\n",
    "# Check Validity of Dates\n",
    "future_dates_original_reg = df_train[df_train['original_reg_date'] > pd.Timestamp.now()]\n",
    "future_dates_reg = df_train[df_train['reg_date'] > pd.Timestamp.now()]\n",
    "\n",
    "# Check Logical Consistency\n",
    "inconsistent_dates = df_train[df_train['reg_date'] < df_train['original_reg_date']]\n",
    "\n",
    "# Output results\n",
    "print(f\"Missing values in 'original_reg_date': {missing_original_reg_date}\")\n",
    "print(f\"Missing values in 'reg_date': {missing_reg_date}\")\n",
    "print(f\"Data type of 'original_reg_date': {original_reg_date_type}\")\n",
    "print(f\"Data type of 'reg_date': {reg_date_type}\")\n",
    "\n",
    "if not future_dates_original_reg.empty:\n",
    "    print(f\"Future dates found in 'original_reg_date': {future_dates_original_reg.shape[0]} entries\")\n",
    "else:\n",
    "    print(\"No future dates found in 'original_reg_date'.\")\n",
    "\n",
    "if not future_dates_reg.empty:\n",
    "    print(f\"Future dates found in 'reg_date': {future_dates_reg.shape[0]} entries\")\n",
    "else:\n",
    "    print(\"No future dates found in 'reg_date'.\")\n",
    "\n",
    "if not inconsistent_dates.empty:\n",
    "    print(f\"Inconsistent dates found: {inconsistent_dates.shape[0]} entries where 'reg_date' is earlier than 'original_reg_date'.\")\n",
    "else:\n",
    "    print(\"All dates are consistent: 'reg_date' is not earlier than 'original_reg_date'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Missing Values\n",
    "missing_vehicle_type = df_train['type_of_vehicle'].isnull().sum()\n",
    "missing_transmission = df_train['transmission'].isnull().sum()\n",
    "\n",
    "# Check Unique Values\n",
    "unique_vehicle_types = df_train['type_of_vehicle'].unique()\n",
    "unique_transmissions = df_train['transmission'].unique()\n",
    "\n",
    "# Check for Consistency\n",
    "# Define expected categories\n",
    "expected_vehicle_types = ['suv', 'luxury sedan', 'mpv', 'mid-sized sedan', 'sports car', 'truck', 'hatchback', 'stationwagon', 'bus/mini bus', 'van']\n",
    "\n",
    "# Check if unique values are in expected categories\n",
    "inconsistent_vehicle_types = [v for v in unique_vehicle_types if v not in expected_vehicle_types]\n",
    "\n",
    "# Output results\n",
    "print(f\"Missing values in 'type_of_vehicle': {missing_vehicle_type}\")\n",
    "print(f\"Missing values in 'transmission': {missing_transmission}\")\n",
    "\n",
    "print(f\"Unique values in 'type_of_vehicle': {unique_vehicle_types}\")\n",
    "print(f\"Unique values in 'transmission': {unique_transmissions}\")\n",
    "\n",
    "if inconsistent_vehicle_types:\n",
    "    print(f\"Inconsistent vehicle types found: {inconsistent_vehicle_types}\")\n",
    "else:\n",
    "    print(\"All vehicle types are consistent with expected categories.\")\n",
    "\n",
    "# Check that vehicle type is consistent with all model-make pairs\n",
    "unique_vehicle_count = (df_train\n",
    "                        .groupby(['make', 'model'])['type_of_vehicle']\n",
    "                        .nunique()\n",
    "                        .reset_index(name='unique_count'))\n",
    "\n",
    "# Step 2: Filter to find combinations with more than one unique vehicle type\n",
    "multiple_types = unique_vehicle_count[unique_vehicle_count['unique_count'] > 1]\n",
    "\n",
    "# Step 3: Check if there are any such combinations\n",
    "if not multiple_types.empty:\n",
    "    print(\"The following model and make combinations map to more than one vehicle type:\")\n",
    "    print(multiple_types)\n",
    "else:\n",
    "    print(\"All model and make combinations map to a single vehicle type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Textual Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df_train.select_dtypes(include=[\"object\"]).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = ['title', 'description', 'category', 'features', 'accessories', 'opc_scheme', 'eco_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud for each text feature\n",
    "for feature in text_features:\n",
    "    text_data = ' '.join(df_train[feature].dropna().tolist())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  \n",
    "    plt.title(f'Wordcloud – {feature}')\n",
    "    plt.savefig(f'visualisations/wordcloud_{feature}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create binary variables from text features\n",
    "\n",
    "# Convert opc scheme to binary for further analysis\n",
    "df_train['opc_scheme'] = df_train['opc_scheme'].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "# Create column for parf v coe cars \n",
    "df_train['parf'] = df_train['category'].apply(lambda x: 1 if 'parf' in x else 0)\n",
    "# Create column for rare & exotic cars\n",
    "df_train['rare'] = df_train['category'].apply(lambda x: 1 if 'rare & exotic' in x else 0)\n",
    "# Create column for vintage cars\n",
    "df_train['vintage'] = df_train['category'].apply(lambda x: 1 if 'vintage' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['opc_scheme', 'parf', 'rare', 'vintage']\n",
    "for col in binary_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df_train[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Fix Missing Values (make) ===\")\n",
    "print(\"Missing values: \",df_train['make'].isna().sum())\n",
    "\n",
    "\n",
    "df_train['make']          = df_train['make'].str.upper()\n",
    "df_train['make']          = df_train['make'].str.replace(' ','').str.strip()\n",
    "df_train['title']         = df_train['title'].str.upper()\n",
    "\n",
    "make_list                   = [make for make in df_train['make'].unique().tolist() if type(make) == str]\n",
    "make_list.sort()\n",
    "print(\"Unique values: \")\n",
    "print(make_list[:20])\n",
    "print()\n",
    "\n",
    "df_train['make_temp']     = df_train['title'].str.split(' ').str[0]\n",
    "df_train['make_temp_similar']     = df_train.apply(lambda x: difflib.get_close_matches(x['make_temp'], make_list, n=1)[0], axis=1)\n",
    "\n",
    "df_train['make']          = df_train['make'].fillna(df_train['make_temp'])\n",
    "df_train                  = df_train.drop(columns = ['make_temp', 'make_temp_similar'])\n",
    "print(\"Missing values (after cleaning): \", df_train['make'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Fix Missing Values (manufactured) ===\")\n",
    "print(\"Missing values: \",df_train['manufactured'].isna().sum())\n",
    "\n",
    "df_train['original_reg_date']         = pd.to_datetime(df_train['original_reg_date'], format = \"%d-%b-%Y\")\n",
    "df_train['reg_date']                  = pd.to_datetime(df_train['reg_date'], format = \"%d-%b-%Y\")\n",
    "\n",
    "df_train['original_reg_date_temp']    = df_train['original_reg_date'].dt.year\n",
    "df_train['reg_date_temp']             = df_train['reg_date'].dt.year\n",
    "df_train['manufactured']              = df_train['manufactured'].fillna(df_train[['original_reg_date_temp','reg_date_temp']].min(axis=1))\n",
    "df_train['manufactured']              = df_train['manufactured'].astype(int).astype(str)\n",
    "df_train                              = df_train.drop(columns = ['original_reg_date_temp', 'reg_date_temp'])\n",
    "print(\"Missing values (after cleaning): \" ,df_train['manufactured'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Fix Missing Values (no_of_owners) ===\")\n",
    "print(\"Missing values: \",df_train['no_of_owners'].isna().sum())\n",
    "print(\"\\nSummary statistics: \", df_train['no_of_owners'].describe())\n",
    "df_train['no_of_owners'] = df_train['no_of_owners'].fillna(df_train['no_of_owners'].median())\n",
    "print(\"\\nMissing values (after cleaning): \" ,df_train['no_of_owners'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fuel type from category\n",
    "fuel_keywords = {\n",
    "    'electric': 'electric',\n",
    "    'hybrid': 'petrol-electric'\n",
    "}\n",
    "\n",
    "def extract_fuel_type(category_text):\n",
    "    category_text = category_text.lower()  \n",
    "    for keyword, fuel_type in fuel_keywords.items():\n",
    "        if keyword in category_text:\n",
    "            return fuel_type\n",
    "    return None\n",
    "\n",
    "# Apply the function to the rows where fuel_type is missing\n",
    "df_train['fuel_type_category_fill'] = df_train['fuel_type']\n",
    "print(f\"Number of missing values for fuel_type (before): {df_train['fuel_type_category_fill'].isna().sum()}\")\n",
    "df_train.loc[df_train['fuel_type'].isna(), 'fuel_type_category_fill'] = df_train['category'].apply(extract_fuel_type)\n",
    "print(f\"Number of missing values for fuel_type (after): {df_train['fuel_type_category_fill'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for fuel type from model make\n",
    "fuel_type_mapping = df_train.groupby(['make', 'model'])['fuel_type'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()\n",
    "fuel_type_dict = dict(zip(zip(fuel_type_mapping['make'], fuel_type_mapping['model']), fuel_type_mapping['fuel_type']))\n",
    "\n",
    "# Define a function to fill missing fuel types\n",
    "def fill_fuel_type(row, fuel_type_dict):\n",
    "    if pd.isna(row['fuel_type']):\n",
    "        return fuel_type_dict.get((row['make'], row['model']), None)\n",
    "    return row['fuel_type']\n",
    "\n",
    "# Apply the function to fill in missing values\n",
    "df_train['fuel_type_model_make_fill'] = df_train['fuel_type']\n",
    "print(f\"Number of missing values for fuel_type (before): {df_train['fuel_type_model_make_fill'].isna().sum()}\")\n",
    "df_train.loc[df_train['fuel_type'].isna(), 'fuel_type_model_make_fill']  = df_train.apply(fill_fuel_type, axis=1, fuel_type_dict=fuel_type_dict)\n",
    "print(f\"Number of missing values for fuel_type (after): {df_train['fuel_type_model_make_fill'].isna().sum()}\")\n",
    "\n",
    "df_train['fuel_type'] = df_train['fuel_type_model_make_fill']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['AGE-current'] = datetime.now().year - df_train['manufactured'].astype(int)\n",
    "\n",
    "\"\"\"Calculate the remaining years of COE based on the vehicle title and registration dates.\"\"\"\n",
    "df_train['title'] = df_train['title'].str.upper()  # Convert titles to uppercase for consistency\n",
    "\n",
    "# Extract COE expiration year from title if it contains 'COE'\n",
    "df_train.loc[df_train['title'].str.contains('COE'), 'coe_temp'] = df_train['title'].str.split(' ').str[-1]\n",
    "df_train['coe_temp1'] = df_train['coe_temp'].str.replace(')', '')  # Clean up extracted year\n",
    "\n",
    "# Convert the cleaned COE year to datetime\n",
    "df_train['coe_temp1'] = pd.to_datetime(df_train['coe_temp1'], format=\"%m/%Y\", errors='coerce')\n",
    "\n",
    "# Calculate the COE end date based on the latest registration date\n",
    "df_train['coe_temp2'] = df_train[['original_reg_date', 'reg_date']].max(axis=1) + pd.offsets.DateOffset(years=10)\n",
    "df_train['coe_enddate'] = df_train['coe_temp1'].fillna(df_train['coe_temp2'])  # Use COE date if available\n",
    "\n",
    "# Calculate remaining age until COE expiration\n",
    "df_train['AGE-remaining'] = df_train.apply(lambda x: dateutil.relativedelta.relativedelta(x['coe_enddate'], datetime.now()).years, axis=1)\n",
    "\n",
    "# Clean up temporary columns used for calculations\n",
    "df_train = df_train.drop(columns=[\"coe_temp\", \"coe_temp1\", \"coe_temp2\", \"coe_enddate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coe end date from title\n",
    "df_train['coe_end'] = df_train['title'].str.extract(r'\\(COE TILL (\\d{2}/\\d{4})\\)')\n",
    "df_train['coe_end'] = pd.to_datetime(df_train['coe_end'], format='%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_info = get_missing_info(df_train=df_train)\n",
    "missing_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_cols = df_train.select_dtypes(include=['int64', 'float64']).columns.drop(['listing_id'])\n",
    "\n",
    "print(\"\\n=== Numerical Features Analysis ===\")\n",
    "print(df_train[numerical_cols].describe())\n",
    "\n",
    "# Check for outliers using IQR method\n",
    "print(\"\\n=== Outlier Analysis (IQR Method) ===\")\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_train[col].quantile(0.25)\n",
    "    Q3 = df_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df_train[(df_train[col] < (Q1 - 1.5 * IQR)) | (df_train[col] > (Q3 + 1.5 * IQR))][col]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"Number of outliers: {len(outliers)}\")\n",
    "        print(f\"Percentage of outliers: {(len(outliers)/len(df_train)*100):.2f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(len(numerical_cols), 2, figsize=(10, 2*len(numerical_cols)))\n",
    "\n",
    "for i, feature in enumerate(numerical_cols):\n",
    "    \n",
    "    # Histogram\n",
    "    sns.histplot(df_train[feature].dropna(), kde=True, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'Histogram of {feature}')\n",
    "    axes[i, 0].set_xlabel(feature)\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(x=df_train[feature].dropna(), ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f'Box Plot of {feature}')\n",
    "    axes[i, 1].set_xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visualisations/distribution_numerical.png\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot correlation matrix of numerical features\"\"\"\n",
    "corr = df_train[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.savefig(\"visualisations/correlation_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strongest correlations with price**\n",
    "- dereg_value (0.91)\n",
    "- arf (0.89)\n",
    "- omv (0.82)\n",
    "- depreciation (0.81)\n",
    "- power (0.70)\n",
    "\n",
    "Focus on these features for initial modelling effort.\n",
    "\n",
    "\n",
    "**Moderate correlations with price**\n",
    "- road_tax (0.52)\n",
    "- engine_cap (0.44)\n",
    "- coe (0.35)\n",
    "- mileage (-0.39)\n",
    "- rare (0.60)\n",
    "\n",
    "**Low correlation with price**\n",
    "- manufactured (0.20)\n",
    "- curb_weight (0.15)\n",
    "- no_of_owners (-0.08)\n",
    "\n",
    "**Strong correlations between features**\n",
    "\n",
    "- omv and arf (0.94)\n",
    "- engine_cap and road_tax (0.94)\n",
    "- power and engine_cap (0.86)\n",
    "\n",
    "Potential multicollinearity which may impact some models (tree-based models are less sensitive). Choose most relevant features or employ dimensionality reduction methods like PCA.\n",
    "Feature engineering to create interaction terms between strongly related features (e.g. power * engine cap)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_corr_price = ['dereg_value', 'arf', 'omv', 'depreciation', 'power']\n",
    "moderate_corr_price = ['road_tax', 'engine_cap', 'coe', 'mileage', 'rare', 'AGE-remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.pairplot(df_train, vars=strong_corr_price + ['price'], hue='parf')\n",
    "plt.title('Features with strong corr with Price Pairplot (by parf)')\n",
    "plt.savefig(\"visualisations/pairplot_strong_corr_by_parf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.pairplot(df_train, vars=strong_corr_price+['price'], hue='rare')\n",
    "plt.title('Features with strong corr with Price Pairplot (by rare)')\n",
    "plt.savefig(\"visualisations/pairplot_strong_corr_by_rare.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.pairplot(df_train, vars=moderate_corr_price + ['price'], hue='parf')\n",
    "plt.title('Features with strong corr with Price Pairplot (by parf)')\n",
    "plt.savefig(\"visualisations/pairplot_mod_corr_by_parf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.pairplot(df_train, vars=moderate_corr_price+['price'], hue='rare')\n",
    "plt.title('Features with strong corr with Price Pairplot (by rare)')\n",
    "plt.savefig(\"visualisations/pairplot_mod_corr_by_rare.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"\\n=== Categorical Features Analysis ===\")\n",
    "for col in categorical_cols:\n",
    "    unique_values = df_train[col].nunique()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Number of unique values: {unique_values}\")\n",
    "    if unique_values < 10:  # Only show value counts for columns with few unique values\n",
    "        print(df_train[col].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- eco_category has the same value for all rows. Drop column.\n",
    "- opc_scheme has 3 unique values. Present value seem to indicate that the car is under OPC scheme. Convert to binary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Analysis (with added binary features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)\n",
    "print(categorical_cols)\n",
    "print(binary_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_and_binary_cols = list(categorical_cols) + binary_columns\n",
    "categorical_and_binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_UNIQUE_VALUES = 30\n",
    "for feature in categorical_and_binary_cols:\n",
    "    if len(df_train[feature].value_counts()) < MAX_UNIQUE_VALUES:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(x=feature, data=df_train, order=df_train[feature].value_counts().index, palette=\"Set3\")\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'visualisations/distribution_categorical_{feature}.png')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Relationship with price\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.boxplot(x=feature, y='price', data=df_train, palette=\"Set3\")\n",
    "        plt.title(f'{feature} vs Price')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'visualisations/price_vs_{feature}.png')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Type of vehicle:\n",
    "    - SUVs, luxury sedans, and sports cars are the most common vehicle types in the dataset. - SUVs are highly represented, with relatively lower price variability compared to luxury sedans and sports cars.\n",
    "    - Truck, station wagon, and bus/mini bus have lower counts, indicating that these vehicle types are less represented in this dataset, possibly niche categories. Generally lower median prices.\n",
    "    - Luxury sedans and sports cars show the highest resale prices, as expected given the premium nature of these vehicles. There are several outliers in these categories, indicating that some cars are priced significantly higher than the rest.\n",
    "    - SUVs have a relatively wide price range, with a number of outliers in the upper range, possibly high-end or luxury SUVs. \n",
    "    \n",
    "- Tranmission type:\n",
    "    - The majority of vehicles in the dataset have automatic transmission.\n",
    "    - Manual transmission vehicles tend to have a lower price range overall compared to automatics. However, there are still some outliers with higher prices, which may correspond to specific models that are rarer or in high demand among enthusiasts (e.g., sports cars with manual transmission).\n",
    "\n",
    "- Fuel type:\n",
    "    - The most common fuel type is diesel, followed by petrol-electric and petrol. This suggests a significant preference for diesel vehicles in the dataset.\n",
    "    - Electric vehicles are less common, while diesel-electric vehicles have the lowest count among the listed fuel types.\n",
    "    - Petrol-electric vehicles also show a significant range in pricing, with some outliers that could represent premium models. This suggests that hybrid vehicles are valued well in the resale market.\n",
    "    - Electric vehicles have a narrower price range compared to diesel and petrol-electric, suggesting that the market for used electric vehicles may not be as established yet, potentially affecting their resale value.\n",
    "    - Note: Many missing values for this feature\n",
    "- opc_scheme:\n",
    "    - Since the feature is highly imbalanced, box plot is not very useful\n",
    "- parf:\n",
    "    - parf cars have a slightly higher range compared to coe cars. \n",
    "- rare:\n",
    "    - Rare cars can command much higher resale prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features = ['original_reg_date','reg_date', 'lifespan', 'coe_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Date Fields Analysis ===\")\n",
    "for col in date_features:\n",
    "\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Unique values sample: {df_train[col].unique()[:5]}\")\n",
    "    # Check for invalid dates\n",
    "    try:\n",
    "        df_train[col] = pd.to_datetime(df_train[col])\n",
    "        print(\"Min date:\", df_train[col].min())\n",
    "        print(\"Max date:\", df_train[col].max())\n",
    "    except:\n",
    "        print(\"Error converting to datetime - possible invalid date formats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(df_train['price'])\n",
    "plt.title('Boxplot of Price to detect outliers')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['price']==max(df_train['price'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.groupby(['make','model'])['price'].describe().sort_values(by='count', ascending=False).to_csv('price_model_make.csv')\n",
    "\n",
    "df_price_breakdown = df_train.groupby(['make','model'])['price'].describe()\n",
    "df_price_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Values Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ENCODE_transmission(df):\n",
    "    \n",
    "    df['TRANSMISSION-manual'] = pd.get_dummies(df['transmission'], drop_first=True, dtype=int)\n",
    "    df                        = df.drop(columns = ['transmission'])\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "def ENCODE_vehtype(df):\n",
    "    \n",
    "    encoder             = ce.BinaryEncoder(cols='type_of_vehicle',return_df=True)\n",
    "    df_temp             = encoder.fit_transform(df['type_of_vehicle']) \n",
    "    df_temp.columns     = [col.replace('type_of_vehicle_', \"TYPE-binenc\") for col in df_temp.columns] \n",
    "\n",
    "    df                  = pd.concat([df, df_temp], axis = 1)\n",
    "    df                  = df.drop(columns = 'type_of_vehicle')\n",
    "\n",
    "    return df\n",
    "\n",
    "def ENCODE_make(df):\n",
    "\n",
    "    encoder             = ce.BinaryEncoder(cols='make',return_df=True)\n",
    "    df_temp             = encoder.fit_transform(df['make']) \n",
    "    df_temp.columns     = [col.replace('make_', \"MAKE-binenc\") for col in df_temp.columns] \n",
    "\n",
    "    df = pd.concat([df, df_temp], axis =1)\n",
    "    df = df.drop(columns = ['make'])\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def ENCODE_fueltype(df):\n",
    "    \n",
    "    encoder         = ce.BinaryEncoder(cols='fuel_type',return_df=True)\n",
    "    df_temp         = encoder.fit_transform(df['fuel_type']) \n",
    "    df_temp.columns = [col.replace('fuel_type_', \"FUEL-binenc\") for col in df_temp.columns] \n",
    "\n",
    "    df = pd.concat([df, df_temp], axis = 1)\n",
    "    df = df.drop(columns = 'fuel_type')\n",
    "\n",
    "    return df\n",
    "\n",
    "def ENCODE_model(df):\n",
    "\n",
    "    df['model']     = df['model'].str.upper()\n",
    "    \n",
    "    encoder         = ce.BinaryEncoder(cols='model',return_df=True)\n",
    "    df_temp         = encoder.fit_transform(df['model']) \n",
    "    df_temp.columns = [col.replace('model_', \"MODEL-binenc\") for col in df_temp.columns] \n",
    "\n",
    "    df = pd.concat([df, df_temp], axis = 1)\n",
    "    df = df.drop(columns = 'model')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['title', 'description', 'original_reg_date', 'category','features', 'accessories', 'fuel_type_category_fill', 'fuel_type_model_make_fill',\n",
    "       'coe_end', 'lifespan', 'eco_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_train.drop(columns=drop_cols).copy()\n",
    "df_encoded = ENCODE_transmission(df_encoded)\n",
    "df_encoded = ENCODE_vehtype(df_encoded)\n",
    "df_encoded = ENCODE_make(df_encoded)\n",
    "df_encoded = ENCODE_model(df_encoded)\n",
    "df_encoded = ENCODE_fueltype(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_CAR_RELATED_COLS = ['TRANSMISSION-manual',\n",
    "                     'TYPE-binenc0',\n",
    "                     'TYPE-binenc1',\n",
    "                     'TYPE-binenc2',\n",
    "                     'TYPE-binenc3',\n",
    "                     'MAKE-binenc0',\n",
    "                     'MAKE-binenc1',\n",
    "                     'MAKE-binenc2',\n",
    "                     'MAKE-binenc3',\n",
    "                     'MAKE-binenc4',\n",
    "                     'MAKE-binenc5',\n",
    "                     'MAKE-binenc6',\n",
    "                     'MODEL-binenc0',\n",
    "                     'MODEL-binenc1',\n",
    "                     'MODEL-binenc2',\n",
    "                     'MODEL-binenc3',\n",
    "                     'MODEL-binenc4',\n",
    "                     'MODEL-binenc5',\n",
    "                     'MODEL-binenc6',\n",
    "                     'MODEL-binenc7',\n",
    "                     'MODEL-binenc8',\n",
    "                     'MODEL-binenc9',]\n",
    "\n",
    "GENERAL_CAR_AND_AGE_RELATED_COLS = GENERAL_CAR_RELATED_COLS + ['AGE-current',\n",
    "                     'AGE-remaining'\n",
    "                    ]\n",
    "\n",
    "\n",
    "def IMPUTENULL_power(df):\n",
    "    related_cols = GENERAL_CAR_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['power']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['power'] = df['power'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "def IMPUTENULL_curbweight(df):\n",
    "    related_cols = GENERAL_CAR_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['curb_weight']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['curb_weight'] = df['curb_weight'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_enginecap(df):\n",
    "    related_cols =  GENERAL_CAR_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['engine_cap']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['engine_cap'] = df['engine_cap'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_depreciation(df):\n",
    "    related_cols =  GENERAL_CAR_AND_AGE_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['depreciation']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['depreciation'] = df['depreciation'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_arf(df):\n",
    "    related_cols =  GENERAL_CAR_AND_AGE_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['arf']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['arf'] = df['arf'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_omv(df):\n",
    "    related_cols =  GENERAL_CAR_AND_AGE_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['omv']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['omv'] = df['omv'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_mileage(df):\n",
    "    related_cols =  ['TRANSMISSION-manual',\n",
    "                     'AGE-current',\n",
    "                     'AGE-remaining'\n",
    "                    ]\n",
    "    df_temp = df[related_cols + ['mileage']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['mileage'] = df['mileage'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_roadtax(df):\n",
    "    related_cols = GENERAL_CAR_AND_AGE_RELATED_COLS\n",
    "    df_temp = df[related_cols + ['road_tax']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['road_tax'] = df['road_tax'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "def IMPUTENULL_deregvalue(df):\n",
    "    related_cols = ['depreciation', 'AGE-remaining', 'omv', 'arf', 'rare']\n",
    "    df_temp = df[related_cols + ['dereg_value']]\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_temp = imputer.fit_transform(df_temp)\n",
    "    df_temp = pd.DataFrame(df_temp[:,-1])\n",
    "    df['dereg_value'] = df['dereg_value'].fillna(df_temp[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_encoded = IMPUTENULL_power(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_curbweight(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_enginecap(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_depreciation(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_omv(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_arf(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_mileage(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_roadtax(df=df_encoded)\n",
    "df_encoded = IMPUTENULL_deregvalue(df=df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['dereg_value_TEMP'] = df_encoded['dereg_value'].fillna(df_encoded['depreciation'] * df_encoded['AGE-remaining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(columns=['reg_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(data_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(data_scaled)\n",
    "\n",
    "distances = np.sort(distances[:, 4], axis=0)\n",
    "plt.plot(distances)\n",
    "plt.ylabel('k-distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "pca = PCA()\n",
    "X_reduced = pca.fit(X_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "print(cumulative_variance)\n",
    "\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1  # +1 because index is 0-based\n",
    "print(f\"Number of components to retain 90% variance: {n_components_90}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')  # Line at 95%\n",
    "plt.axvline(x=n_components_90, color='g', linestyle='--')  # Line indicating number of components\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components_90)  # Use the number of components determined earlier\n",
    "pca_data = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Determine the `eps` Parameter using k-distance graph\n",
    "# Use NearestNeighbors to find the k-nearest distances\n",
    "k = 5  # Common choice for DBSCAN; can adjust based on data characteristics\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(pca_data)\n",
    "distances, indices = neighbors.kneighbors(pca_data)\n",
    "# Get the distances to the k-th nearest neighbor\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_distances)\n",
    "plt.title('K-Distance Graph')\n",
    "plt.xlabel('Data Points sorted by Distance to their {}-th Nearest Neighbor'.format(k))\n",
    "plt.ylabel('Distance')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DBSCAN\n",
    "db = DBSCAN(eps=5, min_samples=5)\n",
    "\n",
    "# Fit the model\n",
    "db.fit(X_scaled)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = db.labels_\n",
    "\n",
    "# Add labels to the original data\n",
    "df_encoded['cluster'] = labels\n",
    "\n",
    "# Identify anomalies (labeled as -1 by DBSCAN)\n",
    "anomalies = df_encoded[df_encoded['cluster'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Cluster'] = labels\n",
    "df_train['Anomaly'] = (labels == -1).astype(int)\n",
    "\n",
    "anomalies_df = df_train[df_train['Cluster']==-1]\n",
    "anomalies_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Cluster'] = labels\n",
    "anomalies_df = df_train[(df_train['Cluster']==-1) \n",
    "                        & (df_train['rare']==0)]\n",
    "print(len(anomalies_df))\n",
    "anomalies_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.pairplot(df_train[df_train['rare']==0], vars=strong_corr_price + ['price'], hue='Anomaly')\n",
    "plt.title('Features with strong corr with Price Pairplot (by Cluster)')\n",
    "# plt.savefig(\"visualisations/pairplot_strong_corr_by_parf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.pairplot(df_train[df_train['rare']==0], vars=moderate_corr_price+['price'], hue='Anomaly')\n",
    "plt.title('Features with strong corr with Price Pairplot (by Cluster)')\n",
    "# plt.savefig(\"visualisations/pairplot_strong_corr_by_rare.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['reg_date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['reg_date']=='1959-05-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['AGE-remaining']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_age_remaning = df_train[df_train['AGE-remaining']<0]\n",
    "neg_age_remaning.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
